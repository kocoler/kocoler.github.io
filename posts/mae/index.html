<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>MAE + MKE 设计 | kocoler's blog</title>
<meta name=keywords content>
<meta name=description content="MAE(MUXI APP ENGINE) 主要分为两个部分：
 基础业务 集群管理   对应 Github 上的 specs Github 上为英文版本
  因为历史原因，项目基本结构并不是 DDD 但是大部分分包也是比较清晰的，虽然并不指望有人维护，这也是代码里要更多考虑 自恢复/差错容忍/其他人为外部运维因素 的原因 部分设计参考阿里成熟的开源项目 kubevela 项目代码清晰，符合普遍的社区标准 但是目前还未开源&mldr; 此介绍文档还在不断更新，如果更新了设计，以及我想更新哈哈哈qwq
 技术选用:
 Mysql 因为本项目大部分数据的数据模型都是关系模型，所以选用了较为熟悉的 RDB Mysql, Gorm 作为 orm 框架 Redis 本项目需要最基本的 业务层缓存 以及 轻量消息队列作为缓冲，所以选用了较为熟悉的 Redis Client-go 管理集群是本项目核心部分，目前团队基本所有服务都部署与 k3s(更轻量的 k8s，减少服务器负载) 管理的集群上，所以选用 client-go 作为项目与集群的交互的 k8s SDK Gin 为本项目的 Web 框架  重点：
 关于集群管理 关于某些场景下一致性的讨论  基础业务 用户系统 最基本的用户系统，以及鉴权 接入团队 OAuth2.0 接口，设计实现 OAuth Interface，符合规范，便于接入其他的 OAuth 认证平台">
<meta name=author content>
<link rel=canonical href=https://kocoler.github.io/posts/mae/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://kocoler.github.io/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://kocoler.github.io/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://kocoler.github.io/favicon-32x32.png>
<link rel=apple-touch-icon href=https://kocoler.github.io/apple-touch-icon.png>
<link rel=mask-icon href=https://kocoler.github.io/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.90.1">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><meta property="og:title" content="MAE + MKE 设计">
<meta property="og:description" content="MAE(MUXI APP ENGINE) 主要分为两个部分：
 基础业务 集群管理   对应 Github 上的 specs Github 上为英文版本
  因为历史原因，项目基本结构并不是 DDD 但是大部分分包也是比较清晰的，虽然并不指望有人维护，这也是代码里要更多考虑 自恢复/差错容忍/其他人为外部运维因素 的原因 部分设计参考阿里成熟的开源项目 kubevela 项目代码清晰，符合普遍的社区标准 但是目前还未开源&mldr; 此介绍文档还在不断更新，如果更新了设计，以及我想更新哈哈哈qwq
 技术选用:
 Mysql 因为本项目大部分数据的数据模型都是关系模型，所以选用了较为熟悉的 RDB Mysql, Gorm 作为 orm 框架 Redis 本项目需要最基本的 业务层缓存 以及 轻量消息队列作为缓冲，所以选用了较为熟悉的 Redis Client-go 管理集群是本项目核心部分，目前团队基本所有服务都部署与 k3s(更轻量的 k8s，减少服务器负载) 管理的集群上，所以选用 client-go 作为项目与集群的交互的 k8s SDK Gin 为本项目的 Web 框架  重点：
 关于集群管理 关于某些场景下一致性的讨论  基础业务 用户系统 最基本的用户系统，以及鉴权 接入团队 OAuth2.0 接口，设计实现 OAuth Interface，符合规范，便于接入其他的 OAuth 认证平台">
<meta property="og:type" content="article">
<meta property="og:url" content="https://kocoler.github.io/posts/mae/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-06-29T21:48:39+00:00">
<meta property="article:modified_time" content="2021-06-29T21:48:39+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="MAE + MKE 设计">
<meta name=twitter:description content="MAE(MUXI APP ENGINE) 主要分为两个部分：
 基础业务 集群管理   对应 Github 上的 specs Github 上为英文版本
  因为历史原因，项目基本结构并不是 DDD 但是大部分分包也是比较清晰的，虽然并不指望有人维护，这也是代码里要更多考虑 自恢复/差错容忍/其他人为外部运维因素 的原因 部分设计参考阿里成熟的开源项目 kubevela 项目代码清晰，符合普遍的社区标准 但是目前还未开源&mldr; 此介绍文档还在不断更新，如果更新了设计，以及我想更新哈哈哈qwq
 技术选用:
 Mysql 因为本项目大部分数据的数据模型都是关系模型，所以选用了较为熟悉的 RDB Mysql, Gorm 作为 orm 框架 Redis 本项目需要最基本的 业务层缓存 以及 轻量消息队列作为缓冲，所以选用了较为熟悉的 Redis Client-go 管理集群是本项目核心部分，目前团队基本所有服务都部署与 k3s(更轻量的 k8s，减少服务器负载) 管理的集群上，所以选用 client-go 作为项目与集群的交互的 k8s SDK Gin 为本项目的 Web 框架  重点：
 关于集群管理 关于某些场景下一致性的讨论  基础业务 用户系统 最基本的用户系统，以及鉴权 接入团队 OAuth2.0 接口，设计实现 OAuth Interface，符合规范，便于接入其他的 OAuth 认证平台">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://kocoler.github.io/posts/"},{"@type":"ListItem","position":3,"name":"MAE + MKE 设计","item":"https://kocoler.github.io/posts/mae/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"MAE + MKE 设计","name":"MAE \u002b MKE 设计","description":"MAE(MUXI APP ENGINE) 主要分为两个部分：\n 基础业务 集群管理   对应 Github 上的 specs Github 上为英文版本\n  因为历史原因，项目基本结构并不是 DDD 但是大部分分包也是比较清晰的，虽然并不指望有人维护，这也是代码里要更多考虑 自恢复/差错容忍/其他人为外部运维因素 的原因 部分设计参考阿里成熟的开源项目 kubevela 项目代码清晰，符合普遍的社区标准 但是目前还未开源\u0026hellip; 此介绍文档还在不断更新，如果更新了设计，以及我想更新哈哈哈qwq\n 技术选用:\n Mysql 因为本项目大部分数据的数据模型都是关系模型，所以选用了较为熟悉的 RDB Mysql, Gorm 作为 orm 框架 Redis 本项目需要最基本的 业务层缓存 以及 轻量消息队列作为缓冲，所以选用了较为熟悉的 Redis Client-go 管理集群是本项目核心部分，目前团队基本所有服务都部署与 k3s(更轻量的 k8s，减少服务器负载) 管理的集群上，所以选用 client-go 作为项目与集群的交互的 k8s SDK Gin 为本项目的 Web 框架  重点：\n 关于集群管理 关于某些场景下一致性的讨论  基础业务 用户系统 最基本的用户系统，以及鉴权 接入团队 OAuth2.0 接口，设计实现 OAuth Interface，符合规范，便于接入其他的 OAuth 认证平台","keywords":[],"articleBody":"MAE(MUXI APP ENGINE) 主要分为两个部分：\n 基础业务 集群管理   对应 Github 上的 specs Github 上为英文版本\n  因为历史原因，项目基本结构并不是 DDD 但是大部分分包也是比较清晰的，虽然并不指望有人维护，这也是代码里要更多考虑 自恢复/差错容忍/其他人为外部运维因素 的原因 部分设计参考阿里成熟的开源项目 kubevela 项目代码清晰，符合普遍的社区标准 但是目前还未开源… 此介绍文档还在不断更新，如果更新了设计，以及我想更新哈哈哈qwq\n 技术选用:\n Mysql 因为本项目大部分数据的数据模型都是关系模型，所以选用了较为熟悉的 RDB Mysql, Gorm 作为 orm 框架 Redis 本项目需要最基本的 业务层缓存 以及 轻量消息队列作为缓冲，所以选用了较为熟悉的 Redis Client-go 管理集群是本项目核心部分，目前团队基本所有服务都部署与 k3s(更轻量的 k8s，减少服务器负载) 管理的集群上，所以选用 client-go 作为项目与集群的交互的 k8s SDK Gin 为本项目的 Web 框架  重点：\n 关于集群管理 关于某些场景下一致性的讨论  基础业务 用户系统 最基本的用户系统，以及鉴权 接入团队 OAuth2.0 接口，设计实现 OAuth Interface，符合规范，便于接入其他的 OAuth 认证平台\n黑名单在服务启动时从 Mysql 数据库 Fan-out 到 Redis 缓存，便于后期使用 黑名单部分作为 协程(goroutinue) 加快项目启动时初始化的速度 可以考虑将 Redis 部分做成布隆过滤器，但是项目目前使用人数较少，未启用\n组织系统 用户可以成为相应组织的成员，组织是根据团队内的应用划分(application) 的 主要用途：\n 同时 application 也用于划分 k8s 集群上的 Namespace 用于隔离不同的 application 的资源 用于在不同操作时进行 RBAC 鉴权控制  服务管理(部署) 每一个服务都属于一个应用(因为基本都是微服务，以及一些单体小服务，都适用于服务的概念)\n传统的部署方法：创建 NS(如果没有) - 创建 Ingress(附带 SSL 相关资源) - 创建 Deployment - 创建 Service\n这里将 Deployment 和 Service 聚合在一起，只给用户暴露一个 服务(Service) 的概念 还有一个 Ingress 路由的概念，其余与 k8s 交互及逻辑部分内部实现\n用户可以 自定义模板(template) (通过 text/template 解析后，绑定到 k8s resource 的各个参数)用于生成部署在服务器上的资源\n具有有默认模板，适用于大部分业务\n并且将通用配置的统一出来，填表部署(填充必须字段)\n子模块：\n 部署记录，也根据路由和服务分为两个部分 特定的 ID 记录用于 回滚 特定版本(集群粒度的控制)，并且 实时（refresh status） 显示每个部署的服务在集群上的状态 至于为什么需要用户手动刷新或者刷新页面，减少服务器负载，如果什么都用最极致的，那肯定 websocket 了，但是太重了，也不需要 也可以 setInterval 1s 定时刷新  部署状态 我们都知道 k8s 的资源是需要创建的，其中 Ingress, Service, Namespce, Deployment 的创建结果都是会同步被 api-server 返回的，那么我们可以假设，如果他创建成功，那么他就是成功的了 事实上，如果我们忽略运维人为删除，那么这个状态就是可以保证的，将状态写入到数据库进行持久化 而 Deployment 下面的 RS, node 则较为复杂，下文也会讨论到\n那么我们可以很方便的将部署状态分为两部分：创建时（api-server）状态和运行时（在集群上）状态\n  创建状态 这里主要考虑的是 Ingress, Namespace 的直接创建结果 如果还未触发第一阶段的回调（创建回调）： 部署中 成功返回：部署成功 失败返回：部署失败\n  集群状态 从项目的角度来说，Ingress, Namespace 之外的概念应该是 Version, 这里就直接对应到 Service + Deployment 两个的状态，为了保证 Service 和 Deployment specs 的一致，不遗漏修改信息，我们选择 diff 一下然后 update/create 而这两个的创建时状态和上面一样 这里主要涉及的是之后的状态：Pending, Healthy, Failed\n  最后，每一步的部署，都会对应一个 RecordID，每个 RecordID 可以对应到对应的 Log Records 来让用户观察到其中部署的细节，像其他 CI 的 Log 一样的效果。 当然这里会考虑到一瞬间大量日志挤爆 Mysql 的情况，这种情况下我们可以加入 MQ 来缓冲一下，使流速均匀。\n服务器、集群   服务器 因为团队所需的服务器也不是很少，因此需要单一的服务器记录系统 业务需要记录服务器的各种配置信息以及过期信息 到期前若干天触发过期提示\n设计是：用户创建 RAM 用户(阿里云概念，可以是其他信息) - RAM 自动导入服务器信息\n抽象 ECS Interface 用于从服务商导入服务器\n  集群\n目前采用的方式是使用集群内 ServiceAccount 的 Token + CA 证书，用于访问集群内的服务(一个集群一个)\n关于集群部分的权限控制需要在集群上改动其权限，从 MAE 中剥离开来，这也是初始化集群的时候应该做的（目前手动，可以考虑 bash 脚本自动 scp 到本地然后走 api 上传）\n  流水线(CI pipeline) 关于团队自建的 Drone CI ，需要将 Build 部分接入到 基础业务 中 设计 CI Interface，用于兼容不同 CI 的 SDK 主要用于将服务的部署部分都集成到一起(用户通过 push gitea 的 tag 触发 drone ci 的 trigger，最后 MAE 一键部署)\n通知系统 具体的通知分发业务后迁移至 MUMS 项目 主要涉及 服务器到期，服务部署状态提示 等\n指标(metrics) 只做服务内各指标监控 暴露出用于监控该服务的内部信息，暴露到 Prometheus，然后接入 Grafana 监控整个服务情况 Tracing 部分正在完善中，可以更好的掌握服务性能等\n集群管理(Cluster Management) 概览 MKE(MUXI K8s Engine) 是独立于 MAE 存在的与 k8s api server 交互的，自己封装 libary(大概算是)\n  MKE with client-go\nMKE 内嵌于 MAE 中的部分 通过 k8s 官方的 SDK client-go 与目标 集群上的 api-server 交互 这一部分主要这几了最基本的(也是业务需要的) 创建，更新，删除 等等功能\n  MKE CRD\n搭配 MAE 使用的 Custom Resource 是一个 Controller 的角色，辅助 MAE 进行整个集群上部署情况的整理\n因为单纯使用 clinet-go 与集群交互对于一些监控状态操作不是那么直接(如：Deployment 的具体状态，同时涉及 Deployment 与 ReplicaSet)\n并且在测试场景遇到了一些 BUG(已经忘了是什么了，好像是在 服务重启时 会 重复触发 事件) 于是直接选择了最开始就想的集群 CRD 方案，让这一部分作为 CRD 运行在各个集群上去帮助我们得知 Deployemnt 的状态，反馈回来\n还有一个原因就是，我们的 MAE 与 集群 交互是走公网的，因为可能会与不同的集群不在同一个地域，所以也要尽可能的减少与集群间的网络流量，并且学生机性能并不好，尤其要减少单个服务器的处理任务（均摊）\n  详细资源 接下来是根据 k8s 中我们需要用到的具体的资源\nNamespace  Resource relations: Namespace  Cluster  Application\n Namespace 作为 K8s 中最直接的隔离资源的概念，非常适合对应到我们业务中的 Application\n  新的 Application\n先根据 Application 的 name(符合 RFC 规定的) 在所有的集群上创建 Namespace\n  新的 Cluster\n根据所有存在的 Application 在新的集群上创建 Namespace\n  这样就保证了所有的 Application 在不同集群上的迁移部署等等（因为这里的集群对所有的服务都是可用的，所以是所有的集群，如果涉及到其他粒度的控制问题，则需要再加详细的控制逻辑）\nIngress  Resource relations: Different env(product/test)  Namespaces  Service  Ingress\n Ingress 作为内部服务暴露到集群与外界交互的一部分，需要用户自定义每个 Route 的细节，并且可以引入 SSL\n  创建流程：\n新的 Ingress 资源 - 在目标集群上 Apply - 如果有错误，直接 Callback\n  如何收集需要部署的目标集群\n因为这里的集群分为 Test 和 Product 两种，每次的部署需要同时考虑这两种 对应关系为： ns - services, ns - ingresses, service - clusters = ingresses - clusters\n  同时，所有的 Ingress 资源在集群中由 name(version) 来区分 Ingress 的 更新，创建，新版本发布，在不同的 版本之间是分开的，但是在不同的集群（生产/测试）上是可以不同的\nService + Deployment = version 讲 K8s 中的 Service 和 Deployment 组合在一起，形成本项目中的 版本（version） 的概念，也就是向用户暴露的概念，用户的所有操作都是对 version 的操作，其中的信息也是直接编写对应 version 的 service template\n  状态\n分为两个部分：\n 部署（deploying）到集群上的状态 已经在集群上的状态 Pending，Success 或者 Failed 并且把这些日志集成到对应部署记录的日志中，通过特定的部署记录访问相关的部署日志 如上述 部署状态 所述    回滚\n对于每一条 DeployRecord, 都可以回滚到相应的版本状态 关于其他的 metadata, 统一 JSON 序列化，每次使用时再反序列化\n  流程\n关系：一个版本对应多个资源对应多个集群 one version - multiple resources – multiple clusters 对应到集群上的关系：\n new deployment - replicaset(s) - pod(s)\n new version - apply on target clusters(service \u0026\u0026 deployment) - callback\n  service\n因为 service 也是只涉及一个单独的创建资源过程，所以这里采用的方法是直接 callback 触发相关流程，也不需要重试等等\n  deployment\n之前的设计: 通过 go-client 内置的一系列 watcher 那一套去接收目标服务器的事件，再处理\n goto deploymentStatusWatcher - replicasetsWacher - podWacher - callback\n 现在的设计: 对于创建资源状态阶段 是可以直接 callback 的 对于在集群上的具体状态是要通过 deploymentWatcher 返回来的状态更新\n    具有默认模板给用户使用\n一些讨论 Callback 很明显，各种因素（主要是响应时间），导致了这里的所有与 集群 的交互过程基本都是异步的\n所以这里使用了 Callback 机制来完成所有的异步回调，更新状态机状态\n场景举例：\n  创建资源 只需要用 MKE 封装的 apply 方法进行创建资源操作，将结果处理函数 这样将 MKE 与 MAE 完全解耦，使得 MAE 某一个资源的后续操作完全自由控制，MKE 只负责回调 具体操作是再次封装 MKE 的各种方法，放在 /service 中，作为 cluster 的一种 service 调用\n如图： CallBack 的 Interface 设计 具有三个主要的函数，分别对应 完成、日志、删除 并不对应 成功/失败 具体逻辑由 service 决定，service 只保证一个服务会成功的写入日志 Log，并且成功的返回部署的状态，并且会在必要的时刻删除该记录（当然这里是软删除）标记不可用\n  一致性 持久化 作为保障一致性最有效的手段就是持久化 每一部分关于资源的一些状态，都需要持久化，并且需要一些策略来保证每个事件对每个状态机的影响，同时需要保证每次交互的 幂等性 这里持久化的内容规定为：所有资源的状态，每次更新都需要同步，DB 部分交给 MySQL 来完成，上层部分通过各个资源的状态机来完成，以及最重要的，每一次更新资源状态的 DeployRecords 通过最新的 DeployRecords, MAE 可以正确的获取到当前达成的状态，如果产生某些状态缺失，则会立即更新，并且如果有多个事件到达，只会更新最新的事件，并且会更新所有以前未完成的状态\n一致性 我们的一致性模型是无主的，可以有若干个 MAE 部署在若干地方 只要保证每个请求打到任意一个节点，那么他的状态就是全局可见的，因此不会有冲突，如果这个节点挂掉，那么他的状态也一定是没有被执行的，那么一定会有新的事件顶替掉这个旧事件 当然我们理想的场景是，有限个 MAE, 每个集群一个 deployment operater 所以我们需要更多的考虑，当旧 MAE 挂掉之后，新的 MAE 起来的时候，看到的状态是否是一致 事实上，在我们实现的模型中，因为 Goroutinue 是无状态的，所以会丢失创建资源的最终状态，或者超时回调，但是这个本身概率就是很小的(api-server 的访问非常迅速) 关于 deployment-operater, 可以将最新的 状态信息发送到目的 MAE 实例，如果没有收到 OK 的 response，那么就认为是无效的，需要重新发送，这里可以加入 二进制指数退避算法 重试 这样，最终的状态一定是不会冲突的，达成了我们可以容忍的弱一致性\n在以上两点的基础上，容错机制也就体现出来了，因为总会有更新的事件去更新上一个挂掉的实例未完成的事件\n为什么把 deployment-operater 剥离出来 上文讨论过具体的思考 同时也是为了方便后面更多的 CRD 资源出现\n配套组件 MUMS MUMS(Muxi Unify Message System)\n用于统一目前可用的 飞书、钉钉 等第三方消息推送服务 用于推送到团队此基础平台的各种通知消息\n目前已有：feishu-bot 用于推送 ci 的构建结果\n… TO BE CONTINUE\n","wordCount":"663","inLanguage":"en","datePublished":"2021-06-29T21:48:39Z","dateModified":"2021-06-29T21:48:39Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://kocoler.github.io/posts/mae/"},"publisher":{"@type":"Organization","name":"kocoler's blog","logo":{"@type":"ImageObject","url":"https://kocoler.github.io/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://kocoler.github.io accesskey=h title="kocoler's blog (Alt + H)">kocoler's blog</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
MAE + MKE 设计
</h1>
<div class=post-meta><span title="2021-06-29 21:48:39 +0000 UTC">June 29, 2021</span>
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#%e5%9f%ba%e7%a1%80%e4%b8%9a%e5%8a%a1 aria-label=基础业务>基础业务</a><ul>
<li>
<a href=#%e7%94%a8%e6%88%b7%e7%b3%bb%e7%bb%9f aria-label=用户系统>用户系统</a></li>
<li>
<a href=#%e7%bb%84%e7%bb%87%e7%b3%bb%e7%bb%9f aria-label=组织系统>组织系统</a></li>
<li>
<a href=#%e6%9c%8d%e5%8a%a1%e7%ae%a1%e7%90%86%e9%83%a8%e7%bd%b2 aria-label=服务管理(部署)><strong>服务管理(部署)</strong></a><ul>
<li>
<a href=#%e9%83%a8%e7%bd%b2%e7%8a%b6%e6%80%81 aria-label=部署状态><strong>部署状态</strong></a></li></ul>
</li>
<li>
<a href=#%e6%9c%8d%e5%8a%a1%e5%99%a8%e9%9b%86%e7%be%a4 aria-label=服务器、集群>服务器、集群</a></li>
<li>
<a href=#%e6%b5%81%e6%b0%b4%e7%ba%bfci-pipeline aria-label="流水线(CI pipeline)">流水线(CI pipeline)</a></li>
<li>
<a href=#%e9%80%9a%e7%9f%a5%e7%b3%bb%e7%bb%9f aria-label=通知系统>通知系统</a></li>
<li>
<a href=#%e6%8c%87%e6%a0%87metrics aria-label=指标(metrics)>指标(metrics)</a></li></ul>
</li>
<li>
<a href=#%e9%9b%86%e7%be%a4%e7%ae%a1%e7%90%86cluster-management aria-label="集群管理(Cluster Management)">集群管理(Cluster Management)</a><ul>
<li>
<a href=#%e6%a6%82%e8%a7%88 aria-label=概览>概览</a></li>
<li>
<a href=#%e8%af%a6%e7%bb%86%e8%b5%84%e6%ba%90 aria-label=详细资源>详细资源</a><ul>
<li>
<a href=#namespace aria-label=Namespace>Namespace</a></li>
<li>
<a href=#ingress aria-label=Ingress>Ingress</a></li>
<li>
<a href=#service--deployment--version aria-label="Service + Deployment =&amp;gt; version">Service + Deployment => version</a></li></ul>
</li></ul>
</li>
<li>
<a href=#%e4%b8%80%e4%ba%9b%e8%ae%a8%e8%ae%ba aria-label=一些讨论>一些讨论</a><ul>
<li>
<a href=#callback aria-label=Callback>Callback</a></li>
<li>
<a href=#%e4%b8%80%e8%87%b4%e6%80%a7 aria-label=一致性>一致性</a><ul>
<li>
<a href=#%e6%8c%81%e4%b9%85%e5%8c%96 aria-label=持久化>持久化</a></li>
<li>
<a href=#%e4%b8%80%e8%87%b4%e6%80%a7-1 aria-label=一致性>一致性</a></li></ul>
</li>
<li>
<a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e6%8a%8a-deployment-operater-%e5%89%a5%e7%a6%bb%e5%87%ba%e6%9d%a5 aria-label="为什么把 deployment-operater 剥离出来">为什么把 deployment-operater 剥离出来</a></li>
<li>
<a href=#%e9%85%8d%e5%a5%97%e7%bb%84%e4%bb%b6 aria-label=配套组件>配套组件</a><ul>
<li>
<a href=#mums aria-label=MUMS>MUMS</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><p>MAE(MUXI APP ENGINE) 主要分为两个部分：</p>
<ul>
<li>基础业务</li>
<li>集群管理</li>
</ul>
<blockquote>
<p>对应 Github 上的 specs
Github 上为英文版本</p>
</blockquote>
<blockquote>
<p>因为历史原因，项目基本结构并不是 DDD
但是大部分分包也是比较清晰的，虽然并不指望有人维护，这也是代码里要更多考虑 自恢复/差错容忍/其他人为外部运维因素 的原因
部分设计参考阿里成熟的开源项目 kubevela
项目代码清晰，符合普遍的社区标准
但是目前还未开源&mldr;
此介绍文档还在不断更新，如果更新了设计，以及我想更新哈哈哈qwq</p>
</blockquote>
<p>技术选用:</p>
<ul>
<li><code>Mysql</code> 因为本项目大部分数据的数据模型都是关系模型，所以选用了较为熟悉的 RDB <code>Mysql</code>, <code>Gorm</code> 作为 <code>orm</code> 框架</li>
<li><code>Redis</code> 本项目需要最基本的 业务层缓存 以及 轻量消息队列作为缓冲，所以选用了较为熟悉的 Redis</li>
<li><code>Client-go</code> 管理集群是本项目核心部分，目前团队基本所有服务都部署与 <code>k3s</code>(更轻量的 k8s，减少服务器负载) 管理的集群上，所以选用 client-go 作为项目与集群的交互的 k8s SDK</li>
<li><code>Gin</code> 为本项目的 Web 框架</li>
</ul>
<p><strong>重点</strong>：</p>
<ul>
<li>关于集群管理</li>
<li>关于某些场景下一致性的讨论</li>
</ul>
<h3 id=基础业务>基础业务<a hidden class=anchor aria-hidden=true href=#基础业务>#</a></h3>
<h4 id=用户系统>用户系统<a hidden class=anchor aria-hidden=true href=#用户系统>#</a></h4>
<p>最基本的用户系统，以及鉴权
接入团队 OAuth2.0 接口，设计实现 <strong>OAuth Interface</strong>，符合规范，便于接入其他的 OAuth 认证平台</p>
<p>黑名单在服务启动时从 Mysql 数据库 <strong>Fan-out</strong> 到 Redis 缓存，便于后期使用
黑名单部分作为 协程(goroutinue) 加快项目启动时初始化的速度
可以考虑将 Redis 部分做成布隆过滤器，但是项目目前使用人数较少，未启用</p>
<h4 id=组织系统>组织系统<a hidden class=anchor aria-hidden=true href=#组织系统>#</a></h4>
<p>用户可以成为相应组织的成员，组织是根据团队内的应用划分(application) 的
主要用途：</p>
<ul>
<li>同时 application 也用于划分 k8s 集群上的 Namespace</li>
<li>用于隔离不同的 application 的资源</li>
<li>用于在不同操作时进行 <strong>RBAC</strong> 鉴权控制</li>
</ul>
<h4 id=服务管理部署><strong>服务管理(部署)</strong><a hidden class=anchor aria-hidden=true href=#服务管理部署>#</a></h4>
<p>每一个服务都属于一个应用(因为基本都是微服务，以及一些单体小服务，都适用于服务的概念)</p>
<p>传统的部署方法：创建 NS(如果没有) -> 创建 Ingress(附带 SSL 相关资源) -> 创建 Deployment -> 创建 Service</p>
<p>这里将 Deployment 和 Service 聚合在一起，只给用户暴露一个 服务(Service) 的概念
还有一个 Ingress 路由的概念，其余与 k8s 交互及逻辑部分内部实现</p>
<p>用户可以 <strong>自定义模板(template)</strong> (通过 text/template 解析后，绑定到 k8s resource 的各个参数)用于生成部署在服务器上的资源</p>
<p>具有有默认模板，适用于大部分业务</p>
<p>并且将通用配置的统一出来，填表部署(填充必须字段)</p>
<p>子模块：</p>
<ul>
<li>部署记录，也根据路由和服务分为两个部分
特定的 ID 记录用于 <strong>回滚</strong> 特定版本(集群粒度的控制)，并且 <em>实时（refresh status）</em> 显示每个部署的服务在集群上的状态
至于为什么需要用户手动刷新或者刷新页面，减少服务器负载，如果什么都用最极致的，那肯定 websocket 了，但是太重了，也不需要
也可以 setInterval 1s 定时刷新</li>
</ul>
<h5 id=部署状态><strong>部署状态</strong><a hidden class=anchor aria-hidden=true href=#部署状态>#</a></h5>
<p>我们都知道 k8s 的资源是需要创建的，其中 Ingress, Service, Namespce, Deployment 的创建结果都是会同步被 api-server 返回的，那么我们可以假设，如果他创建成功，那么他就是成功的了
事实上，如果我们忽略运维人为删除，那么这个状态就是可以保证的，将状态写入到数据库进行持久化
而 Deployment 下面的 RS, node 则较为复杂，下文也会讨论到</p>
<p>那么我们可以很方便的将部署状态分为两部分：创建时（api-server）状态和运行时（在集群上）状态</p>
<ul>
<li>
<p>创建状态
这里主要考虑的是 Ingress, Namespace 的直接创建结果
如果还未触发第一阶段的回调（创建回调）： 部署中
成功返回：部署成功
失败返回：部署失败</p>
</li>
<li>
<p>集群状态
从项目的角度来说，Ingress, Namespace 之外的概念应该是 Version, 这里就直接对应到 Service + Deployment 两个的状态，为了保证 Service 和 Deployment specs 的一致，不遗漏修改信息，我们选择 diff 一下然后 update/create
而这两个的创建时状态和上面一样
这里主要涉及的是之后的状态：Pending, Healthy, Failed</p>
</li>
</ul>
<p>最后，每一步的部署，都会对应一个 RecordID，每个 RecordID 可以对应到对应的 <strong>Log Records</strong> 来让用户观察到其中部署的细节，像其他 CI 的 Log 一样的效果。
当然这里会考虑到一瞬间大量日志挤爆 Mysql 的情况，这种情况下我们可以加入 MQ 来缓冲一下，使流速均匀。</p>
<h4 id=服务器集群>服务器、集群<a hidden class=anchor aria-hidden=true href=#服务器集群>#</a></h4>
<ul>
<li>
<p>服务器
因为团队所需的服务器也不是很少，因此需要单一的服务器记录系统
业务需要记录服务器的各种配置信息以及过期信息
到期前若干天触发过期提示</p>
<p>设计是：用户创建 RAM 用户(阿里云概念，可以是其他信息) -> RAM 自动导入服务器信息</p>
<p>抽象 <strong>ECS Interface</strong> 用于从服务商导入服务器</p>
</li>
<li>
<p>集群</p>
<p>目前采用的方式是使用集群内 ServiceAccount 的 Token + CA 证书，用于访问集群内的服务(一个集群一个)</p>
<p>关于集群部分的权限控制需要在集群上改动其权限，从 MAE 中剥离开来，这也是初始化集群的时候应该做的（目前手动，可以考虑 bash 脚本自动 scp 到本地然后走 api 上传）</p>
</li>
</ul>
<h4 id=流水线ci-pipeline>流水线(CI pipeline)<a hidden class=anchor aria-hidden=true href=#流水线ci-pipeline>#</a></h4>
<p>关于团队自建的 Drone CI ，需要将 Build 部分接入到 基础业务 中
设计 <strong>CI Interface</strong>，用于兼容不同 CI 的 SDK
主要用于将服务的部署部分都集成到一起(用户通过 push gitea 的 tag 触发 drone ci 的 trigger，最后 MAE 一键部署)</p>
<h4 id=通知系统>通知系统<a hidden class=anchor aria-hidden=true href=#通知系统>#</a></h4>
<p>具体的通知分发业务后迁移至 MUMS 项目
主要涉及 服务器到期，服务部署状态提示 等</p>
<h4 id=指标metrics>指标(metrics)<a hidden class=anchor aria-hidden=true href=#指标metrics>#</a></h4>
<p>只做服务内各指标监控
暴露出用于监控该服务的内部信息，暴露到 Prometheus，然后接入 Grafana 监控整个服务情况
Tracing 部分正在完善中，可以更好的掌握服务性能等</p>
<h3 id=集群管理cluster-management>集群管理(Cluster Management)<a hidden class=anchor aria-hidden=true href=#集群管理cluster-management>#</a></h3>
<h4 id=概览>概览<a hidden class=anchor aria-hidden=true href=#概览>#</a></h4>
<p>MKE(MUXI K8s Engine) 是独立于 MAE 存在的与 k8s api server 交互的，自己封装 libary(大概算是)</p>
<ul>
<li>
<p><strong>MKE with client-go</strong></p>
<p>MKE 内嵌于 MAE 中的部分
通过 k8s 官方的 SDK client-go 与目标 集群上的 api-server 交互
这一部分主要这几了最基本的(也是业务需要的) 创建，更新，删除 等等功能</p>
</li>
<li>
<p><strong>MKE CRD</strong></p>
<p>搭配 MAE 使用的 Custom Resource
是一个 Controller 的角色，辅助 MAE 进行整个集群上部署情况的整理</p>
<p>因为单纯使用 clinet-go 与集群交互对于一些监控状态操作不是那么直接(如：Deployment 的具体状态，同时涉及 Deployment 与 ReplicaSet)</p>
<p>并且在测试场景遇到了一些 BUG(已经忘了是什么了，好像是在 服务重启时 会 重复触发 事件) 于是直接选择了最开始就想的集群 CRD 方案，让这一部分作为 CRD 运行在各个集群上去帮助我们得知 Deployemnt 的状态，反馈回来</p>
<p>还有一个原因就是，我们的 MAE 与 集群 交互是走公网的，因为可能会与不同的集群不在同一个地域，所以也要尽可能的<strong>减少与集群间的网络流量</strong>，并且学生机性能并不好，尤其要<strong>减少单个服务器的处理任务</strong>（均摊）</p>
</li>
</ul>
<h4 id=详细资源>详细资源<a hidden class=anchor aria-hidden=true href=#详细资源>#</a></h4>
<p>接下来是根据 k8s 中我们需要用到的具体的资源</p>
<h5 id=namespace>Namespace<a hidden class=anchor aria-hidden=true href=#namespace>#</a></h5>
<blockquote>
<p>Resource relations:
Namespace &lt;&mdash;> Cluster &lt;&mdash;> Application</p>
</blockquote>
<p>Namespace 作为 K8s 中最直接的隔离资源的概念，非常适合对应到我们业务中的 Application</p>
<ul>
<li>
<p>新的 Application</p>
<p>先根据 Application 的 name(符合 RFC 规定的) 在<strong>所有</strong>的集群上创建 Namespace</p>
</li>
<li>
<p>新的 Cluster</p>
<p>根据<strong>所有</strong>存在的 Application 在新的集群上创建 Namespace</p>
</li>
</ul>
<p>这样就保证了所有的 Application 在不同集群上的迁移部署等等（因为这里的集群对所有的服务都是可用的，所以是所有的集群，如果涉及到其他粒度的控制问题，则需要再加详细的控制逻辑）</p>
<h5 id=ingress>Ingress<a hidden class=anchor aria-hidden=true href=#ingress>#</a></h5>
<blockquote>
<p>Resource relations:
Different env(product/test) &lt;&mdash;> Namespaces &lt;&mdash;> Service &lt;&mdash;> Ingress</p>
</blockquote>
<p>Ingress 作为内部服务暴露到集群与外界交互的一部分，需要用户自定义每个 Route 的细节，并且可以引入 SSL</p>
<ul>
<li>
<p>创建流程：</p>
<p>新的 Ingress 资源 -> 在目标集群上 Apply -> 如果有错误，直接 Callback</p>
</li>
<li>
<p>如何收集需要部署的目标集群</p>
<p>因为这里的集群分为 Test 和 Product 两种，每次的部署需要同时考虑这两种
对应关系为：
ns -> services, ns -> ingresses, service -> clusters
=>
ingresses -> clusters</p>
</li>
</ul>
<p>同时，所有的 Ingress 资源在集群中由 name(version) 来区分
Ingress 的 更新，创建，新版本发布，在不同的 版本之间是分开的，但是在不同的集群（生产/测试）上是可以不同的</p>
<h5 id=service--deployment--version>Service + Deployment => version<a hidden class=anchor aria-hidden=true href=#service--deployment--version>#</a></h5>
<p>讲 K8s 中的 Service 和 Deployment 组合在一起，形成本项目中的 版本（version） 的概念，也就是向用户暴露的概念，用户的所有操作都是对 version 的操作，其中的信息也是直接编写对应 version 的 service template</p>
<ul>
<li>
<p>状态</p>
<p>分为两个部分：</p>
<ul>
<li>部署（deploying）到集群上的状态</li>
<li>已经在集群上的状态
Pending，Success 或者 Failed
并且把这些日志集成到对应部署记录的日志中，通过特定的部署记录访问相关的部署日志
如上述 部署状态 所述</li>
</ul>
</li>
<li>
<p>回滚</p>
<p>对于每一条 DeployRecord, 都可以回滚到相应的版本状态
关于其他的 metadata, 统一 JSON 序列化，每次使用时再反序列化</p>
</li>
<li>
<p>流程</p>
<p>关系：一个版本对应多个资源对应多个集群
one version -> multiple resources &ndash;> multiple clusters
对应到集群上的关系：</p>
<blockquote>
<p>new deployment -> replicaset(s) -> pod(s)</p>
</blockquote>
<p>new version -> apply on target clusters(service && deployment) -> callback</p>
<ul>
<li>
<p>service</p>
<p>因为 service 也是只涉及一个单独的创建资源过程，所以这里采用的方法是直接 callback 触发相关流程，也不需要重试等等</p>
</li>
<li>
<p>deployment</p>
<p>之前的设计:
通过 go-client 内置的一系列 <strong><code>watcher</code></strong> 那一套去接收目标服务器的事件，再处理</p>
<blockquote>
<p>goto deploymentStatusWatcher -> replicasetsWacher -> podWacher -> callback</p>
</blockquote>
<p>现在的设计:
对于创建资源状态阶段 是可以直接 callback 的
对于在集群上的具体状态是要通过 deploymentWatcher 返回来的状态更新</p>
</li>
</ul>
</li>
</ul>
<p>具有默认模板给用户使用</p>
<h3 id=一些讨论>一些讨论<a hidden class=anchor aria-hidden=true href=#一些讨论>#</a></h3>
<h4 id=callback>Callback<a hidden class=anchor aria-hidden=true href=#callback>#</a></h4>
<p>很明显，各种因素（主要是响应时间），导致了这里的所有与 集群 的交互过程基本都是异步的</p>
<p>所以这里使用了 Callback 机制来完成所有的异步回调，更新状态机状态</p>
<p>场景举例：</p>
<ul>
<li>
<p>创建资源
只需要用 MKE 封装的 apply 方法进行创建资源操作，将结果处理函数
这样将 MKE 与 MAE 完全解耦，使得 MAE 某一个资源的后续操作完全自由控制，MKE 只负责回调
具体操作是再次封装 MKE 的各种方法，放在 <code>/service</code> 中，作为 <code>cluster</code> 的一种 <code>service</code> 调用</p>
<p>如图：
<img loading=lazy src=https://s2.loli.net/2021/12/14/IKJUkWreLyOG3Ec.png alt="图 1">
<br>
CallBack 的 Interface 设计
具有三个主要的函数，分别对应 完成、日志、删除
并不对应 成功/失败
具体逻辑由 service 决定，service 只保证一个服务会成功的写入日志 Log，并且成功的返回部署的状态，并且会在必要的时刻删除该记录（当然这里是软删除）标记不可用</p>
</li>
</ul>
<h4 id=一致性>一致性<a hidden class=anchor aria-hidden=true href=#一致性>#</a></h4>
<h5 id=持久化>持久化<a hidden class=anchor aria-hidden=true href=#持久化>#</a></h5>
<p>作为保障一致性最有效的手段就是持久化
每一部分关于资源的一些状态，都需要持久化，并且需要一些策略来保证每个事件对每个状态机的影响，同时需要保证每次交互的 幂等性
这里持久化的内容规定为：所有资源的状态，每次更新都需要同步，DB 部分交给 MySQL 来完成，上层部分通过各个资源的状态机来完成，以及最重要的，每一次更新资源状态的 <strong>DeployRecords</strong>
通过最新的 DeployRecords, MAE 可以正确的获取到当前达成的状态，如果产生某些状态缺失，则会立即更新，并且<strong>如果有多个事件到达，只会更新最新的事件，并且会更新所有以前未完成的状态</strong></p>
<h5 id=一致性-1>一致性<a hidden class=anchor aria-hidden=true href=#一致性-1>#</a></h5>
<p>我们的一致性模型是无主的，可以有若干个 MAE 部署在若干地方
只要保证每个请求打到任意一个节点，那么他的状态就是全局可见的，因此不会有冲突，如果这个节点挂掉，那么他的状态也一定是没有被执行的，那么一定会有新的事件顶替掉这个旧事件
当然我们理想的场景是，有限个 MAE, 每个集群一个 deployment operater
所以我们需要更多的考虑，当旧 MAE 挂掉之后，新的 MAE 起来的时候，看到的状态是否是一致
事实上，在我们实现的模型中，因为 Goroutinue 是无状态的，所以会丢失创建资源的最终状态，或者超时回调，但是这个本身概率就是很小的(api-server 的访问非常迅速)
关于 deployment-operater, 可以将最新的 状态信息发送到目的 MAE 实例，如果没有收到 OK 的 response，那么就认为是无效的，需要重新发送，这里可以加入 二进制指数退避算法 重试
这样，最终的状态一定是不会冲突的，达成了我们可以容忍的弱一致性</p>
<p>在以上两点的基础上，容错机制也就体现出来了，因为总会有更新的事件去更新上一个挂掉的实例未完成的事件</p>
<h4 id=为什么把-deployment-operater-剥离出来>为什么把 deployment-operater 剥离出来<a hidden class=anchor aria-hidden=true href=#为什么把-deployment-operater-剥离出来>#</a></h4>
<p>上文讨论过具体的思考
同时也是为了方便后面更多的 CRD 资源出现</p>
<h4 id=配套组件>配套组件<a hidden class=anchor aria-hidden=true href=#配套组件>#</a></h4>
<h5 id=mums>MUMS<a hidden class=anchor aria-hidden=true href=#mums>#</a></h5>
<p>MUMS(Muxi Unify Message System)</p>
<p>用于统一目前可用的 飞书、钉钉 等第三方消息推送服务
用于推送到团队此基础平台的各种通知消息</p>
<p>目前已有：feishu-bot 用于推送 ci 的构建结果</p>
<p>&mldr;
TO BE CONTINUE</p>
</div>
<footer class=post-footer>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://kocoler.github.io>kocoler's blog</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>